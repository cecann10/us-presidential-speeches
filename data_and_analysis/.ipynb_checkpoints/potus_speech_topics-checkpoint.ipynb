{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presidential Speeches: Topic Modeling\n",
    "This notebook is used for creating topics for the text on 991 Presidental speeches that span all US Presidents from George Washington to Donald Trump mid-term 2019.\n",
    "\n",
    "This notebook is building off EDA, Pre-Prossesing & Sentiment work done in `potus_speech_eda_sentiment.ipynb` found in the same folder as this worksheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Install modules and import libraries to run this notebook >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install gensim if needed by un-hashing and running:\n",
    "# pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import TreebankWordTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download out-of-nltk-box stop words -- they'll be used later\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickle of file of with clean Presidential speech transcripts'\n",
    "#  in the document-term matrix\n",
    "\n",
    "with open('pickle/transcript_cv_dtm.pickle','rb') as read_file:\n",
    "    transcripts_dtm = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "### Topic Modeling: Attempt #1\n",
    "**LDA with Count Vectorizer.**</br>\n",
    "First basic attempt at topic modeling to set baseline of what we'll need to likely improve upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the document-term matrix into a term-document matrix by transposing it\n",
    "\n",
    "transcripts_tdm = transcripts_dtm.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37928, 991)"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify shape correct\n",
    "\n",
    "transcripts_tdm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn document-term matrix into gensim format to work with LDS model\n",
    "\n",
    "sparse_counts = scipy.sparse.csr_matrix(transcripts_tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open previously pickled cv to create dictionary for gensim\n",
    "\n",
    "with open('pickle/cv_stop.pickle','rb') as read_file:\n",
    "    cv = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for gensim\n",
    "\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"states\" + 0.011*\"government\" + 0.008*\"united\" + 0.006*\"congress\" + 0.005*\"public\" + 0.005*\"country\" + 0.004*\"people\" + 0.004*\"shall\" + 0.004*\"great\" + 0.004*\"state\"'),\n",
       " (1,\n",
       "  '0.010*\"people\" + 0.007*\"world\" + 0.005*\"american\" + 0.005*\"new\" + 0.005*\"president\" + 0.005*\"america\" + 0.005*\"years\" + 0.004*\"time\" + 0.004*\"peace\" + 0.004*\"country\"')]"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 2\n",
    "\n",
    "lda1_2 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=20)\n",
    "lda1_2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"people\" + 0.007*\"world\" + 0.006*\"president\" + 0.005*\"new\" + 0.005*\"american\" + 0.005*\"america\" + 0.005*\"years\" + 0.005*\"time\" + 0.004*\"peace\" + 0.004*\"know\"'),\n",
       " (1,\n",
       "  '0.015*\"states\" + 0.011*\"government\" + 0.010*\"united\" + 0.007*\"congress\" + 0.005*\"state\" + 0.005*\"public\" + 0.005*\"shall\" + 0.005*\"country\" + 0.004*\"people\" + 0.004*\"great\"'),\n",
       " (2,\n",
       "  '0.009*\"government\" + 0.005*\"congress\" + 0.005*\"states\" + 0.005*\"great\" + 0.004*\"year\" + 0.004*\"law\" + 0.004*\"people\" + 0.004*\"country\" + 0.004*\"american\" + 0.004*\"united\"')]"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "\n",
    "lda1_3 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=20)\n",
    "lda1_3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for num_topics = 6\n",
    "# Uping to 6 and doing 50 passes to give this the best chance possible\n",
    "\n",
    "lda1_4 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=6, passes=50)\n",
    "lda1_4.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "### Topic Modeling: Attempt #2\n",
    "**LDA with Count Vectorizer with more tuned parameters.**</br>\n",
    "Lemmatizing the text.  Adding stop words.  Using more parameters for vectorization to tune output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to lemmatize the text\n",
    "\n",
    "def lem_text(text, tokenizer, stemmer):\n",
    "    cleaned_text = []\n",
    "    for word in text:\n",
    "        cleaned_words = []\n",
    "        for word in tokenizer.tokenize(word):\n",
    "            stem_word = stemmer.lemmatize(word)\n",
    "            cleaned_words.append(stem_word)\n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the cleaned transcripts\n",
    "\n",
    "transcripts_clean_rd2_lem = lem_text(transcripts_clean_rd2.Transcript, TreebankWordTokenizer(), WordNetLemmatizer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional stop words that I want to remove to improve results of my topics\n",
    "#  note words continuously were added to this list after seeing results from topics\n",
    "\n",
    "add_stop_words = ['government','know','want','thats','mr',\n",
    "                  'going','year','make','shall','let', 'subject',\n",
    "                  'say', 'think','way','president','said'\n",
    "                 ]\n",
    "\n",
    "# Add new stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clean text\n",
    "\n",
    "with open('pickle/transcripts_clean_rd2.pickle','rb') as read_file:\n",
    "    transcripts_clean_rd2 = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this LDA model attempt, we're still using CountVectorizer, but will add extra parameters to:\n",
    "- add new stop words\n",
    "- set max df to get rid of too commonly used words\n",
    "- set min df to get rid of words not used often enough to impact topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abundant</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accept</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>worthy</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yield</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  ability  able  abroad  absence  absolute  absolutely  abundant  \\\n",
       "0        0        0     0       0        0         0           0         0   \n",
       "1        0        0     0       0        0         0           0         0   \n",
       "2        0        0     0       1        0         0           0         0   \n",
       "3        0        0     1       1        0         0           0         2   \n",
       "4        0        0     0       0        0         0           0         0   \n",
       "\n",
       "   abuse  accept  ...  worth  worthy  written  wrong  yes  yesterday  yield  \\\n",
       "0      0       0  ...      0       0        0      0    0          0      0   \n",
       "1      0       0  ...      0       0        0      0    0          0      0   \n",
       "2      0       0  ...      0       1        0      0    0          0      0   \n",
       "3      0       0  ...      0       0        0      0    0          0      0   \n",
       "4      0       0  ...      0       0        1      0    0          0      0   \n",
       "\n",
       "   york  young  youre  \n",
       "0     0      0      0  \n",
       "1     0      0      0  \n",
       "2     0      0      0  \n",
       "3     0      0      0  \n",
       "4     1      1      0  \n",
       "\n",
       "[5 rows x 1919 columns]"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using new parameters\n",
    "\n",
    "cv2 = CountVectorizer(stop_words=stop_words, min_df=.1, max_df=.8)\n",
    "transcript_cv2 = cv2.fit_transform(transcripts_clean_rd2_lem)\n",
    "transcript_cv_dtm_v2 = pd.DataFrame(transcript_cv2.toarray(), columns=cv2.get_feature_names())\n",
    "transcript_cv_dtm_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpus2 = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(transcript_cv_dtm_v2.T))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2word2 = dict((v, k) for k, v in cv2.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"american\" + 0.012*\"world\" + 0.009*\"america\" + 0.008*\"new\" + 0.007*\"peace\" + 0.006*\"war\" + 0.006*\"work\" + 0.006*\"right\" + 0.005*\"life\" + 0.005*\"today\"'),\n",
       " (1,\n",
       "  '0.010*\"congress\" + 0.010*\"law\" + 0.008*\"power\" + 0.007*\"public\" + 0.006*\"act\" + 0.006*\"duty\" + 0.005*\"present\" + 0.005*\"war\" + 0.005*\"right\" + 0.005*\"citizen\"')]"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 2\n",
    "\n",
    "lda2_2 = models.LdaModel(corpus=corpus2, id2word=id2word2, num_topics=2, passes=20)\n",
    "lda2_2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"congress\" + 0.010*\"law\" + 0.009*\"power\" + 0.008*\"public\" + 0.007*\"act\" + 0.007*\"duty\" + 0.006*\"constitution\" + 0.006*\"citizen\" + 0.006*\"right\" + 0.006*\"war\"'),\n",
       " (1,\n",
       "  '0.014*\"american\" + 0.012*\"world\" + 0.009*\"america\" + 0.008*\"new\" + 0.007*\"peace\" + 0.006*\"war\" + 0.006*\"right\" + 0.006*\"work\" + 0.005*\"life\" + 0.005*\"today\"'),\n",
       " (2,\n",
       "  '0.008*\"congress\" + 0.008*\"law\" + 0.006*\"american\" + 0.006*\"public\" + 0.005*\"work\" + 0.005*\"service\" + 0.005*\"national\" + 0.005*\"business\" + 0.004*\"condition\" + 0.004*\"department\"')]"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "\n",
    "lda2_3 = models.LdaModel(corpus=corpus2, id2word=id2word2, num_topics=3, passes=10)\n",
    "lda2_3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"law\" + 0.012*\"power\" + 0.010*\"constitution\" + 0.010*\"congress\" + 0.009*\"right\" + 0.008*\"act\" + 0.008*\"duty\" + 0.007*\"citizen\" + 0.006*\"public\" + 0.006*\"war\"'),\n",
       " (1,\n",
       "  '0.015*\"american\" + 0.010*\"america\" + 0.009*\"new\" + 0.009*\"work\" + 0.008*\"job\" + 0.007*\"tax\" + 0.007*\"congress\" + 0.006*\"right\" + 0.006*\"need\" + 0.006*\"child\"'),\n",
       " (2,\n",
       "  '0.010*\"congress\" + 0.008*\"public\" + 0.008*\"law\" + 0.006*\"present\" + 0.005*\"service\" + 0.005*\"department\" + 0.005*\"power\" + 0.005*\"duty\" + 0.004*\"condition\" + 0.004*\"act\"'),\n",
       " (3,\n",
       "  '0.018*\"world\" + 0.013*\"peace\" + 0.012*\"american\" + 0.012*\"war\" + 0.008*\"america\" + 0.008*\"new\" + 0.007*\"men\" + 0.007*\"freedom\" + 0.007*\"force\" + 0.006*\"life\"')]"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "\n",
    "lda2_4 = models.LdaModel(corpus=corpus2, id2word=id2word2, num_topics=4, passes=20)\n",
    "lda2_4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"congress\" + 0.007*\"law\" + 0.006*\"public\" + 0.006*\"power\" + 0.005*\"act\" + 0.004*\"duty\" + 0.004*\"present\" + 0.004*\"citizen\" + 0.004*\"war\" + 0.003*\"treaty\"'),\n",
       " (1,\n",
       "  '0.009*\"law\" + 0.006*\"men\" + 0.005*\"right\" + 0.005*\"power\" + 0.004*\"public\" + 0.004*\"business\" + 0.004*\"work\" + 0.004*\"american\" + 0.004*\"constitution\" + 0.003*\"man\"'),\n",
       " (2,\n",
       "  '0.013*\"president\" + 0.006*\"congress\" + 0.006*\"american\" + 0.004*\"world\" + 0.004*\"problem\" + 0.004*\"program\" + 0.004*\"policy\" + 0.004*\"new\" + 0.004*\"national\" + 0.004*\"federal\"'),\n",
       " (3,\n",
       "  '0.011*\"american\" + 0.010*\"america\" + 0.007*\"new\" + 0.007*\"world\" + 0.005*\"work\" + 0.005*\"right\" + 0.005*\"job\" + 0.004*\"president\" + 0.004*\"child\" + 0.004*\"way\"'),\n",
       " (4,\n",
       "  '0.012*\"world\" + 0.012*\"peace\" + 0.011*\"war\" + 0.006*\"american\" + 0.006*\"men\" + 0.006*\"force\" + 0.005*\"freedom\" + 0.005*\"free\" + 0.005*\"new\" + 0.004*\"right\"')]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 5\n",
    "\n",
    "lda2_5 = models.LdaModel(corpus=corpus2, id2word=id2word2, num_topics=5, passes=20)\n",
    "lda2_5.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways from Attempt #2 LDA w/CV parameter models:\n",
    "- **lda2_4**: Seems the easiest to draw themes from.  With more (5 topics) starts to get a bit more abstract.  This will be considered for final topic selection.  Updated to run with 30 passes to refine results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "### Topic Modeling: Attempt #3\n",
    "**LDA with TFIDF with more tuned parameters.**</br>\n",
    "Lemmatizing the text.  Adding stop words.  Using more parameters for vectorization to tune output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abundant</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accept</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>worthy</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yield</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030715</td>\n",
       "      <td>0.037082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039271</td>\n",
       "      <td>0.031724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1922 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  ability      able    abroad  absence  absolute  absolutely  \\\n",
       "0      0.0      0.0  0.000000  0.000000      0.0       0.0         0.0   \n",
       "1      0.0      0.0  0.000000  0.000000      0.0       0.0         0.0   \n",
       "2      0.0      0.0  0.000000  0.051571      0.0       0.0         0.0   \n",
       "3      0.0      0.0  0.030715  0.037082      0.0       0.0         0.0   \n",
       "4      0.0      0.0  0.000000  0.000000      0.0       0.0         0.0   \n",
       "\n",
       "   abundant  abuse  accept  ...  worth    worthy   written  wrong  yes  \\\n",
       "0   0.00000    0.0     0.0  ...    0.0  0.000000  0.000000    0.0  0.0   \n",
       "1   0.00000    0.0     0.0  ...    0.0  0.000000  0.000000    0.0  0.0   \n",
       "2   0.00000    0.0     0.0  ...    0.0  0.064086  0.000000    0.0  0.0   \n",
       "3   0.11377    0.0     0.0  ...    0.0  0.000000  0.000000    0.0  0.0   \n",
       "4   0.00000    0.0     0.0  ...    0.0  0.000000  0.044687    0.0  0.0   \n",
       "\n",
       "   yesterday  yield      york     young  youre  \n",
       "0        0.0    0.0  0.000000  0.000000    0.0  \n",
       "1        0.0    0.0  0.000000  0.000000    0.0  \n",
       "2        0.0    0.0  0.000000  0.000000    0.0  \n",
       "3        0.0    0.0  0.000000  0.000000    0.0  \n",
       "4        0.0    0.0  0.039271  0.031724    0.0  \n",
       "\n",
       "[5 rows x 1922 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using TFIDF\n",
    "#   using lemmatized words as done in Attempt #2\n",
    "\n",
    "tf1 = TfidfVectorizer(stop_words=stop_words, min_df=.1, max_df=.8)\n",
    "transcript_tf1 = tf1.fit_transform(transcripts_clean_rd2_lem)\n",
    "transcript_cv_dtm_tf1 = pd.DataFrame(transcript_tf1.toarray(), columns=tf1.get_feature_names())\n",
    "transcript_cv_dtm_tf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpus3 = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(transcript_cv_dtm_tf1.T))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2word3 = dict((v, k) for k, v in tf1.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"american\" + 0.007*\"world\" + 0.006*\"america\" + 0.004*\"new\" + 0.004*\"peace\" + 0.004*\"president\" + 0.004*\"war\" + 0.004*\"today\" + 0.004*\"life\" + 0.003*\"work\"'),\n",
       " (1,\n",
       "  '0.006*\"law\" + 0.005*\"congress\" + 0.004*\"public\" + 0.004*\"power\" + 0.004*\"duty\" + 0.004*\"constitution\" + 0.003*\"treaty\" + 0.003*\"act\" + 0.003*\"citizen\" + 0.003*\"present\"')]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 2\n",
    "\n",
    "tf1_2 = models.LdaModel(corpus=corpus3, id2word=id2word3, num_topics=2, passes=20)\n",
    "tf1_2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "Looking very similiar to the CountVectorizer options, but we'll try higher number of topics to see if that changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"law\" + 0.001*\"president\" + 0.001*\"indian\" + 0.001*\"said\" + 0.001*\"congress\" + 0.001*\"territory\" + 0.001*\"person\" + 0.001*\"jurisdiction\" + 0.001*\"american\" + 0.001*\"power\"'),\n",
       " (1,\n",
       "  '0.001*\"president\" + 0.001*\"america\" + 0.001*\"day\" + 0.001*\"oath\" + 0.001*\"peace\" + 0.001*\"person\" + 0.001*\"world\" + 0.001*\"american\" + 0.001*\"act\" + 0.001*\"proclamation\"'),\n",
       " (2,\n",
       "  '0.007*\"american\" + 0.007*\"world\" + 0.006*\"america\" + 0.004*\"new\" + 0.004*\"peace\" + 0.004*\"president\" + 0.004*\"war\" + 0.004*\"today\" + 0.004*\"life\" + 0.003*\"freedom\"'),\n",
       " (3,\n",
       "  '0.006*\"law\" + 0.005*\"congress\" + 0.005*\"public\" + 0.004*\"power\" + 0.004*\"duty\" + 0.004*\"constitution\" + 0.003*\"act\" + 0.003*\"treaty\" + 0.003*\"citizen\" + 0.003*\"present\"')]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "\n",
    "tf1_4 = models.LdaModel(corpus=corpus3, id2word=id2word3, num_topics=4, passes=20)\n",
    "tf1_4.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "Definitely now more distinction from what came out of CountVectorizer options, although numbers all seem very low except for a few.  Not certain this will be best option, but will continue to explore.  Let's try taking it down to 3 topics for a middle-ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"president\" + 0.001*\"day\" + 0.001*\"american\" + 0.001*\"prayer\" + 0.001*\"peace\" + 0.001*\"public\" + 0.001*\"religious\" + 0.001*\"america\" + 0.001*\"right\" + 0.001*\"men\"'),\n",
       " (1,\n",
       "  '0.007*\"american\" + 0.007*\"world\" + 0.006*\"america\" + 0.004*\"new\" + 0.004*\"peace\" + 0.004*\"president\" + 0.004*\"war\" + 0.004*\"today\" + 0.004*\"life\" + 0.003*\"work\"'),\n",
       " (2,\n",
       "  '0.006*\"law\" + 0.005*\"congress\" + 0.005*\"public\" + 0.004*\"power\" + 0.004*\"duty\" + 0.004*\"constitution\" + 0.004*\"act\" + 0.004*\"treaty\" + 0.003*\"citizen\" + 0.003*\"present\"')]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "\n",
    "tf1_3 = models.LdaModel(corpus=corpus3, id2word=id2word3, num_topics=3, passes=20)\n",
    "tf1_3.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "Still pretty low perentages and also don't see as clear of topics coming out to this.  Will not likely use models that came out of Attempt #3 with TFIDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "### Topic Modeling: Attempt #4\n",
    "**LDA with Count Vectorizer with tuned parameters + bigrams**</br>\n",
    "Keeping parameters used in Attempt #2, but now also adding in option for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abundant</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accept</th>\n",
       "      <th>...</th>\n",
       "      <th>worthy</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yield</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>young people</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1981 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  ability  able  abroad  absence  absolute  absolutely  abundant  \\\n",
       "0        0        0     0       0        0         0           0         0   \n",
       "1        0        0     0       0        0         0           0         0   \n",
       "2        0        0     0       1        0         0           0         0   \n",
       "3        0        0     1       1        0         0           0         2   \n",
       "4        0        0     0       0        0         0           0         0   \n",
       "\n",
       "   abuse  accept  ...  worthy  written  wrong  yes  yesterday  yield  york  \\\n",
       "0      0       0  ...       0        0      0    0          0      0     0   \n",
       "1      0       0  ...       0        0      0    0          0      0     0   \n",
       "2      0       0  ...       1        0      0    0          0      0     0   \n",
       "3      0       0  ...       0        0      0    0          0      0     0   \n",
       "4      0       0  ...       0        1      0    0          0      0     1   \n",
       "\n",
       "   young  young people  youre  \n",
       "0      0             0      0  \n",
       "1      0             0      0  \n",
       "2      0             0      0  \n",
       "3      0             0      0  \n",
       "4      1             0      0  \n",
       "\n",
       "[5 rows x 1981 columns]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix, now adding bigram option\n",
    "\n",
    "cv3 = CountVectorizer(stop_words=stop_words, ngram_range=(1,2), min_df=.1, max_df=.8)\n",
    "transcript_cv3 = cv3.fit_transform(transcripts_clean_rd2_lem)\n",
    "transcript_cv_dtm_v3 = pd.DataFrame(transcript_cv3.toarray(), columns=cv3.get_feature_names())\n",
    "transcript_cv_dtm_v3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpus4 = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(transcript_cv_dtm_v3.T))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2word4 = dict((v, k) for k, v in cv3.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"american\" + 0.011*\"world\" + 0.009*\"america\" + 0.008*\"new\" + 0.007*\"peace\" + 0.006*\"war\" + 0.006*\"work\" + 0.006*\"right\" + 0.005*\"life\" + 0.005*\"today\"'),\n",
       " (1,\n",
       "  '0.010*\"congress\" + 0.009*\"law\" + 0.007*\"power\" + 0.007*\"public\" + 0.006*\"act\" + 0.006*\"duty\" + 0.005*\"present\" + 0.005*\"war\" + 0.005*\"right\" + 0.005*\"citizen\"')]"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 2\n",
    "\n",
    "lda4_2 = models.LdaModel(corpus=corpus4, id2word=id2word4, num_topics=2, passes=20)\n",
    "lda4_2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "- No bigrams came out in the topics and looks similar to results without bigrams tried in Attempt #2.  We'll try increasing size of topics to see if that reveals any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"law\" + 0.013*\"constitution\" + 0.012*\"power\" + 0.011*\"president\" + 0.010*\"right\" + 0.009*\"congress\" + 0.009*\"act\" + 0.007*\"union\" + 0.007*\"duty\" + 0.006*\"authority\"'),\n",
       " (1,\n",
       "  '0.014*\"american\" + 0.014*\"world\" + 0.011*\"america\" + 0.009*\"new\" + 0.009*\"peace\" + 0.008*\"war\" + 0.007*\"president\" + 0.006*\"life\" + 0.006*\"right\" + 0.006*\"today\"'),\n",
       " (2,\n",
       "  '0.010*\"congress\" + 0.008*\"public\" + 0.008*\"law\" + 0.006*\"power\" + 0.006*\"present\" + 0.005*\"duty\" + 0.005*\"war\" + 0.005*\"treaty\" + 0.005*\"citizen\" + 0.005*\"act\"'),\n",
       " (3,\n",
       "  '0.009*\"american\" + 0.009*\"congress\" + 0.009*\"business\" + 0.009*\"work\" + 0.008*\"tax\" + 0.008*\"president\" + 0.007*\"federal\" + 0.006*\"need\" + 0.006*\"national\" + 0.006*\"program\"')]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "\n",
    "lda4_4 = models.LdaModel(corpus=corpus4, id2word=id2word4, num_topics=4, passes=20)\n",
    "lda4_4.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "- Still no bigrams, but is taking a slightly different approach than Attempt #2 for topics.  Could be interesting to test.  Will continue to up topics to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"law\" + 0.012*\"right\" + 0.010*\"constitution\" + 0.009*\"men\" + 0.008*\"question\" + 0.007*\"court\" + 0.007*\"man\" + 0.007*\"congress\" + 0.007*\"act\" + 0.006*\"person\"'),\n",
       " (1,\n",
       "  '0.016*\"american\" + 0.012*\"america\" + 0.010*\"new\" + 0.009*\"president\" + 0.008*\"work\" + 0.008*\"job\" + 0.006*\"tax\" + 0.006*\"world\" + 0.006*\"right\" + 0.006*\"congress\"'),\n",
       " (2,\n",
       "  '0.018*\"world\" + 0.014*\"peace\" + 0.013*\"war\" + 0.011*\"american\" + 0.009*\"president\" + 0.007*\"force\" + 0.007*\"freedom\" + 0.007*\"new\" + 0.007*\"america\" + 0.006*\"men\"'),\n",
       " (3,\n",
       "  '0.009*\"congress\" + 0.008*\"law\" + 0.007*\"department\" + 0.007*\"american\" + 0.007*\"report\" + 0.006*\"work\" + 0.006*\"service\" + 0.005*\"secretary\" + 0.005*\"legislation\" + 0.005*\"increase\"'),\n",
       " (4,\n",
       "  '0.011*\"congress\" + 0.010*\"power\" + 0.009*\"public\" + 0.008*\"duty\" + 0.007*\"law\" + 0.007*\"act\" + 0.007*\"citizen\" + 0.007*\"war\" + 0.006*\"treaty\" + 0.006*\"present\"'),\n",
       " (5,\n",
       "  '0.009*\"business\" + 0.009*\"congress\" + 0.008*\"national\" + 0.008*\"public\" + 0.006*\"law\" + 0.006*\"federal\" + 0.005*\"power\" + 0.005*\"industry\" + 0.005*\"present\" + 0.005*\"need\"')]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 6\n",
    "\n",
    "lda4_6 = models.LdaModel(corpus=corpus4, id2word=id2word4, num_topics=6, passes=20)\n",
    "lda4_6.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "- Some interesting topics starting to appear here.  A few are a few fuzzy, so we'll try to play around with topic size more -- but this is a strong consideration for topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"law\" + 0.015*\"constitution\" + 0.015*\"right\" + 0.013*\"question\" + 0.010*\"congress\" + 0.009*\"house\" + 0.008*\"election\" + 0.008*\"slavery\" + 0.008*\"act\" + 0.007*\"union\"'),\n",
       " (1,\n",
       "  '0.014*\"world\" + 0.013*\"peace\" + 0.009*\"vietnam\" + 0.009*\"american\" + 0.009*\"soviet\" + 0.009*\"war\" + 0.007*\"force\" + 0.007*\"new\" + 0.006*\"military\" + 0.006*\"security\"'),\n",
       " (2,\n",
       "  '0.019*\"world\" + 0.016*\"war\" + 0.013*\"american\" + 0.012*\"peace\" + 0.012*\"men\" + 0.010*\"life\" + 0.009*\"america\" + 0.009*\"freedom\" + 0.007*\"right\" + 0.007*\"free\"'),\n",
       " (3,\n",
       "  '0.009*\"law\" + 0.008*\"congress\" + 0.008*\"work\" + 0.007*\"business\" + 0.006*\"service\" + 0.006*\"public\" + 0.006*\"national\" + 0.006*\"american\" + 0.005*\"commission\" + 0.005*\"legislation\"'),\n",
       " (4,\n",
       "  '0.017*\"american\" + 0.013*\"america\" + 0.010*\"new\" + 0.009*\"job\" + 0.009*\"work\" + 0.007*\"tax\" + 0.007*\"child\" + 0.007*\"world\" + 0.006*\"help\" + 0.006*\"need\"'),\n",
       " (5,\n",
       "  '0.016*\"public\" + 0.016*\"power\" + 0.009*\"duty\" + 0.009*\"bank\" + 0.008*\"constitution\" + 0.007*\"congress\" + 0.007*\"law\" + 0.007*\"citizen\" + 0.006*\"union\" + 0.006*\"present\"'),\n",
       " (6,\n",
       "  '0.012*\"congress\" + 0.008*\"law\" + 0.008*\"treaty\" + 0.007*\"act\" + 0.007*\"war\" + 0.006*\"duty\" + 0.006*\"citizen\" + 0.006*\"power\" + 0.005*\"territory\" + 0.005*\"public\"')]"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 7\n",
    "\n",
    "lda4_7 = models.LdaModel(corpus=corpus4, id2word=id2word4, num_topics=7, passes=30)\n",
    "lda4_7.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"right\" + 0.011*\"men\" + 0.010*\"constitution\" + 0.010*\"man\" + 0.009*\"law\" + 0.008*\"life\" + 0.008*\"free\" + 0.008*\"slavery\" + 0.007*\"principle\" + 0.007*\"question\"'),\n",
       " (1,\n",
       "  '0.016*\"american\" + 0.011*\"america\" + 0.010*\"job\" + 0.009*\"new\" + 0.009*\"work\" + 0.009*\"tax\" + 0.007*\"congress\" + 0.007*\"child\" + 0.006*\"family\" + 0.006*\"right\"'),\n",
       " (2,\n",
       "  '0.019*\"law\" + 0.014*\"act\" + 0.012*\"officer\" + 0.011*\"person\" + 0.010*\"power\" + 0.009*\"duty\" + 0.009*\"congress\" + 0.009*\"authority\" + 0.007*\"department\" + 0.007*\"constitution\"'),\n",
       " (3,\n",
       "  '0.012*\"congress\" + 0.008*\"law\" + 0.007*\"department\" + 0.006*\"increase\" + 0.006*\"report\" + 0.006*\"public\" + 0.005*\"american\" + 0.005*\"service\" + 0.005*\"legislation\" + 0.005*\"present\"'),\n",
       " (4,\n",
       "  '0.013*\"vietnam\" + 0.011*\"american\" + 0.010*\"war\" + 0.008*\"south\" + 0.008*\"peace\" + 0.007*\"world\" + 0.006*\"question\" + 0.006*\"believe\" + 0.005*\"policy\" + 0.005*\"action\"'),\n",
       " (5,\n",
       "  '0.009*\"business\" + 0.009*\"law\" + 0.008*\"work\" + 0.008*\"men\" + 0.007*\"national\" + 0.007*\"public\" + 0.007*\"power\" + 0.006*\"american\" + 0.006*\"labor\" + 0.006*\"need\"'),\n",
       " (6,\n",
       "  '0.023*\"world\" + 0.015*\"peace\" + 0.012*\"american\" + 0.012*\"america\" + 0.012*\"war\" + 0.011*\"new\" + 0.009*\"freedom\" + 0.008*\"force\" + 0.006*\"today\" + 0.006*\"security\"'),\n",
       " (7,\n",
       "  '0.011*\"congress\" + 0.010*\"power\" + 0.010*\"public\" + 0.007*\"citizen\" + 0.007*\"war\" + 0.007*\"duty\" + 0.007*\"treaty\" + 0.006*\"law\" + 0.006*\"present\" + 0.006*\"act\"')]"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 8\n",
    "\n",
    "lda4_8 = models.LdaModel(corpus=corpus4, id2word=id2word4, num_topics=8, passes=30)\n",
    "lda4_8.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"life\" + 0.012*\"right\" + 0.011*\"world\" + 0.011*\"men\" + 0.009*\"american\" + 0.008*\"man\" + 0.008*\"america\" + 0.007*\"freedom\" + 0.007*\"day\" + 0.007*\"citizen\"'),\n",
       " (1,\n",
       "  '0.012*\"power\" + 0.011*\"congress\" + 0.010*\"public\" + 0.009*\"law\" + 0.008*\"duty\" + 0.008*\"act\" + 0.007*\"war\" + 0.007*\"citizen\" + 0.006*\"constitution\" + 0.005*\"present\"'),\n",
       " (2,\n",
       "  '0.010*\"congress\" + 0.008*\"law\" + 0.007*\"department\" + 0.006*\"service\" + 0.006*\"report\" + 0.005*\"secretary\" + 0.005*\"american\" + 0.005*\"public\" + 0.005*\"treaty\" + 0.005*\"present\"'),\n",
       " (3,\n",
       "  '0.021*\"constitution\" + 0.018*\"slavery\" + 0.015*\"law\" + 0.014*\"right\" + 0.013*\"question\" + 0.013*\"slave\" + 0.012*\"union\" + 0.012*\"territory\" + 0.010*\"congress\" + 0.009*\"free\"'),\n",
       " (4,\n",
       "  '0.017*\"american\" + 0.014*\"america\" + 0.011*\"new\" + 0.009*\"work\" + 0.008*\"job\" + 0.008*\"world\" + 0.007*\"child\" + 0.006*\"right\" + 0.006*\"help\" + 0.006*\"family\"'),\n",
       " (5,\n",
       "  '0.010*\"question\" + 0.009*\"dont\" + 0.008*\"congress\" + 0.008*\"believe\" + 0.008*\"governor\" + 0.007*\"house\" + 0.006*\"administration\" + 0.006*\"problem\" + 0.006*\"day\" + 0.006*\"secretary\"'),\n",
       " (6,\n",
       "  '0.014*\"law\" + 0.009*\"men\" + 0.008*\"work\" + 0.007*\"business\" + 0.007*\"court\" + 0.006*\"congress\" + 0.006*\"power\" + 0.006*\"public\" + 0.005*\"good\" + 0.005*\"condition\"'),\n",
       " (7,\n",
       "  '0.011*\"congress\" + 0.009*\"federal\" + 0.008*\"national\" + 0.008*\"business\" + 0.007*\"economic\" + 0.007*\"public\" + 0.007*\"industry\" + 0.007*\"price\" + 0.006*\"american\" + 0.006*\"world\"'),\n",
       " (8,\n",
       "  '0.020*\"world\" + 0.017*\"peace\" + 0.017*\"war\" + 0.012*\"american\" + 0.010*\"force\" + 0.007*\"new\" + 0.007*\"soviet\" + 0.006*\"freedom\" + 0.006*\"military\" + 0.006*\"men\"')]"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 9\n",
    "\n",
    "lda4_9 = models.LdaModel(corpus=corpus4, id2word=id2word4, num_topics=9, passes=20)\n",
    "lda4_9.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "- With 9 topics things start to lose focus, but 8 and especially 7 has an interesting mix.  8 is getting a bit too narrow in certain topics though (e.g. vietnam war) so 7 is going to be what will be focused on.  Updated number of passes on the model to refine results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "### Topic Modeling: Attempt #5\n",
    "**LSA with Count Vectorizer** </br>\n",
    "While LDA normally works best with large text -- such as presidential speeches -- and was why that model was first used, we'll also test LSA to see if chance it performs better than our LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abundant</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accept</th>\n",
       "      <th>...</th>\n",
       "      <th>worthy</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yield</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>young people</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1986 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  ability  able  abroad  absence  absolute  absolutely  abundant  \\\n",
       "0        0        0     0       0        0         0           0         0   \n",
       "1        0        0     0       0        0         0           0         0   \n",
       "2        0        0     0       1        0         0           0         0   \n",
       "3        0        0     1       1        0         0           0         2   \n",
       "4        0        0     0       0        0         0           0         0   \n",
       "\n",
       "   abuse  accept  ...  worthy  written  wrong  yes  yesterday  yield  york  \\\n",
       "0      0       0  ...       0        0      0    0          0      0     0   \n",
       "1      0       0  ...       0        0      0    0          0      0     0   \n",
       "2      0       0  ...       1        0      0    0          0      0     0   \n",
       "3      0       0  ...       0        0      0    0          0      0     0   \n",
       "4      0       0  ...       0        1      0    0          0      0     1   \n",
       "\n",
       "   young  young people  youre  \n",
       "0      0             0      0  \n",
       "1      0             0      0  \n",
       "2      0             0      0  \n",
       "3      0             0      0  \n",
       "4      1             0      0  \n",
       "\n",
       "[5 rows x 1986 columns]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using parameters that worked well for LDA as starting point\n",
    "\n",
    "cv5 = CountVectorizer(stop_words=stop_words, ngram_range=(1,2), min_df=.1, max_df=.8)\n",
    "transcript_cv5 = cv5.fit_transform(transcripts_clean_rd2_lem)\n",
    "transcript_cv_dtm_v5 = pd.DataFrame(transcript_cv5.toarray(), columns=cv5.get_feature_names())\n",
    "transcript_cv_dtm_v5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26345906, 0.09600439])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run in LSA model for 2 topics and then print variance ratio\n",
    "\n",
    "lsa1_2 = TruncatedSVD(2)\n",
    "doc_topic_lsa1_2 = lsa1_2.fit_transform(transcript_cv_dtm_v5)\n",
    "lsa1_2.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to capture the words under each topic\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "congress, law, american, power, public, new, war, right, president, act\n",
      "\n",
      "Topic  1\n",
      "american, president, world, america, job, new, tax, dont, help, program\n"
     ]
    }
   ],
   "source": [
    "# Examine first 10 words that fit under the topics\n",
    "\n",
    "display_topics(lsa1_2, cv5.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26345906, 0.09600439, 0.04510286, 0.03413083])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run in LSA model for 4 topics and then print variance ratio\n",
    "\n",
    "lsa1_4 = TruncatedSVD(4)\n",
    "doc_topic_lsa1_4 = lsa1_4.fit_transform(transcript_cv_dtm_v5)\n",
    "lsa1_4.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "congress, law, american, power, public, new, war, right, president, act\n",
      "\n",
      "Topic  1\n",
      "american, president, world, america, job, new, tax, dont, help, program\n",
      "\n",
      "Topic  2\n",
      "slavery, slave, constitution, compromise, territory, right, principle, union, free, man\n",
      "\n",
      "Topic  3\n",
      "president, governor, dont, constitution, question, said, secretary, power, senate, bank\n"
     ]
    }
   ],
   "source": [
    "# Examine first 10 words that fit under the topics\n",
    "\n",
    "display_topics(lsa1_4, cv5.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "- Starting to see some topics in these LSA models, but LDA was providing more ways to distinguish the topics and are easier to interpret for the speeches, so will continue with LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "### Topic Modeling: Attempt #6\n",
    "**NMF with Count Vectorizer** </br>\n",
    "As noted with LSA attempt, while LDA normally works best with large text -- such as presidential speeches -- and was why that model was first used, we'll also test NMF to see if chance it performs better than our LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abundant</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accept</th>\n",
       "      <th>...</th>\n",
       "      <th>worthy</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yield</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>young people</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1986 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  ability  able  abroad  absence  absolute  absolutely  abundant  \\\n",
       "0        0        0     0       0        0         0           0         0   \n",
       "1        0        0     0       0        0         0           0         0   \n",
       "2        0        0     0       1        0         0           0         0   \n",
       "3        0        0     1       1        0         0           0         2   \n",
       "4        0        0     0       0        0         0           0         0   \n",
       "\n",
       "   abuse  accept  ...  worthy  written  wrong  yes  yesterday  yield  york  \\\n",
       "0      0       0  ...       0        0      0    0          0      0     0   \n",
       "1      0       0  ...       0        0      0    0          0      0     0   \n",
       "2      0       0  ...       1        0      0    0          0      0     0   \n",
       "3      0       0  ...       0        0      0    0          0      0     0   \n",
       "4      0       0  ...       0        1      0    0          0      0     1   \n",
       "\n",
       "   young  young people  youre  \n",
       "0      0             0      0  \n",
       "1      0             0      0  \n",
       "2      0             0      0  \n",
       "3      0             0      0  \n",
       "4      1             0      0  \n",
       "\n",
       "[5 rows x 1986 columns]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using parameters that worked well for LDA as starting point\n",
    "\n",
    "cv6 = CountVectorizer(stop_words=stop_words, ngram_range=(1,2), min_df=.1, max_df=.8)\n",
    "transcript_cv6 = cv6.fit_transform(transcripts_clean_rd2_lem)\n",
    "transcript_cv_dtm_v6 = pd.DataFrame(transcript_cv6.toarray(), columns=cv6.get_feature_names())\n",
    "transcript_cv_dtm_v6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "congress, treaty, war, mexico, public, citizen, report, present, secretary, new\n",
      "\n",
      "Topic  1\n",
      "american, america, new, job, work, tax, child, congress, family, need\n",
      "\n",
      "Topic  2\n",
      "slavery, slave, compromise, right, territory, principle, free, law, man, question\n",
      "\n",
      "Topic  3\n",
      "president, question, dont, governor, said, believe, thing, like, tax, problem\n",
      "\n",
      "Topic  4\n",
      "world, peace, war, american, force, america, freedom, new, men, life\n",
      "\n",
      "Topic  5\n",
      "power, constitution, law, public, congress, duty, act, bank, right, union\n",
      "\n",
      "Topic  6\n",
      "law, work, business, men, american, court, national, public, congress, department\n",
      "\n",
      "Topic  7\n",
      "examination, service, person, commission, rule, place, officer, appointment, test, general\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "nmf1_4 = NMF(8)\n",
    "doc_topic_nmf1_4 = nmf1_4.fit_transform(transcript_cv6)\n",
    "\n",
    "display_topics(nmf1_4, cv6.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "- Had to increase number of topics to start seeing any sort of distinction and there is some similiarity of what is seen in the past models for topics, but LDA still coming out ahead in terms of allowing for being distinction of topics and understanding weight of words to use to create topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "## Mapping Topics to Each Speech\n",
    "To see what speech was ranked with which first ranking topic and also ensure a proper distribution of the topics across the speeches, we'll map the topics to the speeches in a master dataframe for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open master dataframe with speech information -- we'll be adding the topics to this\n",
    "\n",
    "potus_speech_master = pd.read_csv('csv/potus_speech_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary first column repeating index\n",
    "\n",
    "potus_speech_master.drop(columns='Unnamed: 0', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-<br/>\n",
    "Now we'll create a few functions that will be used to give us the 'top topic' (greatest weight) for each document in our LDA models that we're considering >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to order topics for each document in decending weight value\n",
    "\n",
    "def all_topics_ordered(all_topics_df): \n",
    "    '''\n",
    "    Takes in a dataframe with topic tuples ([topic],[weight of topic])\n",
    "    that are in individual columns for each row/document in the dataframe.  \n",
    "    For every row/document the function creates a dataframe\n",
    "    of all topic tuples for that row/document and sorts them by the \n",
    "    descending weight value of the topic, meaning the first\n",
    "    tuple listed will have the greatest weight of association\n",
    "    to the row/document.\n",
    "    '''\n",
    "    all_speech_topics = []\n",
    "    for index, row in all_topics_df.iterrows(): \n",
    "        speech_topics = []\n",
    "        for topic in row:\n",
    "            if topic != None:\n",
    "                speech_topics.append(topic)\n",
    "        speech_topics.sort(key=lambda x:x[1], reverse=True)\n",
    "        all_speech_topics.append(speech_topics)\n",
    "    return pd.DataFrame(all_speech_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to place topics in the master dataframe for speeches\n",
    "\n",
    "def first_topic(topics, master_df):\n",
    "    '''\n",
    "    Function takes in topic speech rankings from model and \n",
    "    places the first topic for each speech in the master\n",
    "    dataframe for the speeches.\n",
    "    ---\n",
    "    Inputs: \n",
    "    -- topics = all speeches' topic rankings\n",
    "    -- master_df = dataframe with all data on speeches\n",
    "    \n",
    "    Output:\n",
    "    Master dataframe with two new columns:\n",
    "    -- Topic: topic category\n",
    "    -- Topic_Percent: percentage that speech fits in that category\n",
    "    \n",
    "    '''\n",
    "    topic_cat = []\n",
    "    topic_percent = []\n",
    "    for x,y in topics[0]:\n",
    "        topic_cat.append(x)\n",
    "        topic_percent.append(y)\n",
    "\n",
    "    master_df['Topic'] = topic_cat\n",
    "    master_df['Topic_Percent'] = topic_percent\n",
    "    \n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-<br>\n",
    "Now we'll look at the topic breakdowns for the topics we're considering using >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Narrowing, Review, & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lda2_4: LDA, CV, 4 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a listing of all of the topic rankings for each speech\n",
    "corpus_transformed_lda2_4 = lda2_4[corpus2]\n",
    "\n",
    "# make it into a dataframe\n",
    "df_speech_topics_lda2_4 = pd.DataFrame(corpus_transformed_lda2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function above to the dataframe created so that \n",
    "#  the topic with the greatest 'weight' will be listed first\n",
    "\n",
    "ordered_topics_lda2_4 = all_topics_ordered(df_speech_topics_lda2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add first topic listing to master dataframe\n",
    "\n",
    "potus_speech_master_lda2_4 = first_topic(ordered_topics_lda2_4, potus_speech_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    320\n",
       "0    256\n",
       "2    217\n",
       "1    198\n",
       "Name: Topic, dtype: int64"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at distribution of topics across speeches\n",
    "\n",
    "potus_speech_master_lda2_4.Topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takaways:\n",
    "- Fairly nice distribution.  This is good, but will see if we get good distribution for additional topics so we can have more disinction between speeches within reason of not getting too narrow or overwhleming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lda4_7: LDA, CV, 7 Topics\n",
    "We'll look at using the 7 topic model that showed nice results with fairly disinct topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a listing of all of the topic rankings for each speech\n",
    "corpus_transformed_lda4_7 = lda4_7[corpus4]\n",
    "\n",
    "# make it into a dataframe\n",
    "df_speech_topics_lda4_7 = pd.DataFrame(corpus_transformed_lda4_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function above to the dataframe created so that \n",
    "#  the topic with the greatest 'weight' will be listed first\n",
    "\n",
    "ordered_topics_lda4_7 = all_topics_ordered(df_speech_topics_lda4_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now pull the first topic listed (one with greatest weight) and \n",
    "#   pull it into our master dataframe with all speeches' information\n",
    "\n",
    "potus_speech_master_lda4_7 = first_topic(ordered_topics_lda4_7, potus_speech_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    235\n",
       "4    196\n",
       "2    183\n",
       "1    139\n",
       "3     95\n",
       "5     93\n",
       "0     50\n",
       "Name: Topic, dtype: int64"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potus_speech_master_lda4_7.Topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice distribution of topics.  Let's assign topic titles to each topic and then add those into the dataframe as well.  We can then examine if the topic names properly respresent the speeches >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column replacing numerical topics with topic descriptions\n",
    "#   these are topic descriptions selected based on words under each topic\n",
    "\n",
    "topic_categories = potus_speech_master_lda4_7['Topic'].replace({\n",
    "    0: 'Law, constitution, & rights', \n",
    "    1: 'World peace with war & force',\n",
    "    2: 'War with American freedom',\n",
    "    3: 'Work and business',\n",
    "    4: 'American jobs and family help & needs',\n",
    "    5: 'Public power and duty',\n",
    "    6: 'Laws, treaties, and action'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for our analysis, we want to add in major historical periods in American history to the master dataframe to see if there are any relationships or trends with speech topics.  They will be as follows:\n",
    "- 1789 - 1799: Establishment of new democratic nation\n",
    "- 1800 - 1860: Settlement and expansion\n",
    "- 1861 - 1865: American Civil War\n",
    "- 1865 - 1890: Reconstruction Era & Guilded Age\n",
    "- 1890 - 1913: Progressive Era\n",
    "- 1914 - 1918: World War I / Progressive Era\n",
    "- 1919 - 1928: Roaring Twenties / Progressive Era\n",
    "- 1929 - 1932: Great Depression\n",
    "- 1933 - 1938: Great Depression/New Deal\n",
    "- 1939 - 1945: World War II\n",
    "- 1946 - 1953: Cold War\n",
    "- 1954 - 1964: Cold War / Civil Rights Movement\n",
    "- 1965 - 1968: Cold & Vietnam Wars / Civil Rights Movement\n",
    "- 1969 - 1972: Cold & Vietnam Wars\n",
    "- 1973 - 1975: Energy Crisis/Cold & Vietnam Wars\n",
    "- 1976 - 1980: Energy Crisis/Cold War\n",
    "- 1981 - 1991: Reagan Era / Cold War\n",
    "- 1992 - 2000: Neoconservative / Dot-Com Era\n",
    "- 2001 - 2006: 'War on Terror'\n",
    "- 2007 - 2009: Great Recession / 'War on Terror'\n",
    "- 2010 - 2019: 'War on Terror'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty new row to input historical periods\n",
    "\n",
    "potus_speech_master_lda4_7['Historical_Period'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add all historical periods for their appropriate date range\n",
    "\n",
    "for index in range(0,len(potus_speech_master_lda4_7)):\n",
    "    if index <= 27:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1789-1799: New Democratic Nation'\n",
    "    elif index > 27 and index <= 209:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1800-1860: Settlement & Expansion'\n",
    "    elif index > 210 and index <= 237:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1861-1865: American Civil War'\n",
    "    elif index > 237 and index <= 337:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1865-1890: Reconstruction Era & Guilded Age'\n",
    "    elif index > 337 and index <= 419:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1890-1913: Progressive Era'\n",
    "    elif index > 419 and index <= 451:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1914-1918: World War I / Progressive Era'\n",
    "    elif index > 451 and index <= 483:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1919-1928: Roaring Twenties / Progressive Era'\n",
    "    elif index > 483 and index <= 512:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1929-1932: Great Depression'\n",
    "    elif index > 512 and index <= 531:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1933-1938: Great Depression/New Deal'\n",
    "    elif index > 531 and index <= 567:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1939-1945: World War II'\n",
    "    elif index > 567 and index <= 584:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1946-1953: Cold War'\n",
    "    elif index > 584 and index <= 652:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1954-1964: Cold War / Civil Rights Movement'\n",
    "    elif index > 652 and index <= 699:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1965-1968: Cold & Vietnam Wars / Civil Rights Movement'\n",
    "    elif index > 699 and index <= 716:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1969-1972: Cold & Vietnam Wars'\n",
    "    elif index > 716 and index <= 736:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1973-1975:: Energy Crisis/Cold & Vietnam Wars'\n",
    "    elif index > 736 and index <= 763:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1976-1980: Energy Crisis/Cold War'\n",
    "    elif index > 763 and index <= 837:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1981-1991: Reagan Era / Cold War'\n",
    "    elif index > 837 and index <= 881:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = '1992-2000: Neoconservative / Dot-Com Era'\n",
    "    elif index > 881 and index <= 911:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = \"2001-2006: 'War on Terror'\"\n",
    "    elif index > 911 and index <= 932:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = \"2007-2009: Great Recession / 'War on Terror'\"\n",
    "    else:\n",
    "        potus_speech_master_lda4_7.loc[index,'Historical_Period'] = \"2010-2019: Ongoing 'War on Terror'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800-1860: Settlement & Expansion                         182\n",
       "1865-1890: Reconstruction Era & Guilded Age               100\n",
       "1890-1913: Progressive Era                                 82\n",
       "1981-1991: Reagan Era / Cold War                           74\n",
       "1954-1964: Cold War / Civil Rights Movement                68\n",
       "2010-2019: Ongoing 'War on Terror'                         59\n",
       "1965-1968: Cold & Vietnam Wars / Civil Rights Movement     47\n",
       "1992-2000: Neoconservative / Dot-Com Era                   44\n",
       "1939-1945: World War II                                    36\n",
       "1914-1918: World War I / Progressive Era                   32\n",
       "1919-1928: Roaring Twenties / Progressive Era              32\n",
       "2001-2006: 'War on Terror'                                 30\n",
       "1929-1932: Great Depression                                29\n",
       "1789-1799: New Democratic Nation                           28\n",
       "1976-1980: Energy Crisis/Cold War                          27\n",
       "1861-1865: American Civil War                              27\n",
       "2007-2009: Great Recession / 'War on Terror'               21\n",
       "1973-1975:: Energy Crisis/Cold & Vietnam Wars              20\n",
       "1933-1938: Great Depression/New Deal                       19\n",
       "1946-1953: Cold War                                        17\n",
       "1969-1972: Cold & Vietnam Wars                             17\n",
       "Name: Historical_Period, dtype: int64"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now see how many speeches fall in each era\n",
    "\n",
    "potus_speech_master_lda4_7['Historical_Period'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPDATE THIS TO INCLUDE THE VERSION OF THE FINAL MODEL SELECTED\n",
    "\n",
    "# Save the master database for all speeches with sentiment & topics as csv\n",
    "#  to be pulled into Tableau for visualization\n",
    "\n",
    "potus_speech_master_lda4_7.to_csv('csv/potus_speech_master_topic_sentiment.csv')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
